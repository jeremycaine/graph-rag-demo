{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c95934-1015-4d6c-9c35-381293a2e846",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RAG Demo: Graph-Enhanced Vector Store with KeybertLinkExtractor\n",
      "================================================================================\n",
      "\n",
      "Initializing watsonx LLM...\n",
      "Connecting to AstraDB vector store...\n",
      "Vector store connected\n",
      "Clearing existing data from vector store...\n",
      "Vector store cleared\n",
      "Loading input file...\n",
      "Loaded 38539 characters\n",
      "Splitting text into chunks...\n",
      "Created 49 chunks\n",
      "\n",
      "Creating documents from chunks...\n",
      "\n",
      "Extracting keywords with KeyBERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cl/pqrbf72558g1rg14tfhm8yt80000gn/T/ipykernel_78539/290220285.py:134: LangChainBetaWarning: The class `KeybertLinkExtractor` is in beta. It is actively being worked on, so the API may change.\n",
      "  keyword_extractor = KeybertLinkExtractor(kind=\"kw\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted keywords for 49 documents\n",
      "\n",
      "Example document with keywords:\n",
      "Chunk 0:\n",
      "Content preview: Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and th...\n",
      "Extracted 5 keywords: ['putin', 'ukrainian', 'russia', 'ukraine', 'vladimir']\n",
      "\n",
      "Adding 49 documents to vector store...\n",
      "Documents added to vector store\n",
      "\n",
      "================================================================================\n",
      "RUNNING COMPARISONS: Standard Vector Search vs GraphRAG\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What did Biden say about Ukraine?\n",
      "================================================================================\n",
      "\n",
      "STANDARD VECTOR SEARCH (similarity only):\n",
      "--------------------------------------------------------------------------------\n",
      " Biden praised the Ukrainian people for their fearlessness, courage, and determination in the face of Russian aggression. He highlighted specific examples of Ukrainians blocking tanks with their bodies and teachers turning into soldiers to defend their homeland. Biden also emphasized the importance of standing with Ukraine and expressed confidence that Putin's war would ultimately weaken Russia and strengthen the rest of the world. He noted the unity among leaders and people worldwide in support of Ukraine and the rising of democracies in the battle against autocracy. Biden also addressed Ukrainian Americans, reaffirming the bond between the two nations and the United States' commitment to standing with them.\n",
      "\n",
      "Retrieved chunks (standard):\n",
      "  Chunk 1 [Keywords: ambassador, ukrainian, courage]\n",
      "  Chunk 0 [Keywords: putin, ukrainian, russia]\n",
      "  Chunk 8 [Keywords: democracies, putin, ukrainian]\n",
      "\n",
      "\n",
      "GRAPHRAG (similarity + graph traversal):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Biden expressed strong support for Ukraine and its people, praising their courage and determination in the face of Russian aggression. He pledged to provide military, economic, and humanitarian assistance to Ukraine, and emphasized that the United States would not engage in conflict with Russian forces in Ukraine but would defend NATO allies in the region. Biden also announced that the U.S. would release 30 million barrels of oil from its Strategic Petroleum Reserve to help blunt gas prices at home and support Ukraine. He highlighted the unity of the free world in holding Russia accountable for its actions and inflicting economic sanctions on the country. Biden also mentioned the efforts to seize the ill-gotten gains of Russian oligarchs and close American airspace to all Russian flights. He reiterated that the U.S. would defend every inch of NATO territory with collective power and that Putin's aggression would ultimately weaken Russia and strengthen the rest of the world.\n",
      "\n",
      "Retrieved chunks (GraphRAG):\n",
      "  Chunk 1 [Keywords: ambassador, ukrainian, courage]\n",
      "  Chunk 0 [Keywords: putin, ukrainian, russia]\n",
      "  Chunk 8 [Keywords: democracies, putin, ukrainian]\n",
      "  Chunk 9 [Keywords: kyiv, america, putin]\n",
      "  Chunk 5 [Keywords: russian, ukrainian, ukrainians]\n",
      "  Chunk 7 [Keywords: russian, putin, sanctions]\n",
      "  Chunk 6 [Keywords: lithuania, putin, ukrainian]\n",
      "  Chunk 3 [Keywords: russian, putin, russia]\n",
      "  Chunk 4 [Keywords: russian, oligarchs, putin]\n",
      "  Chunk 2 [Keywords: diplomacy, putin, ukraine]\n",
      "  Chunk 13 [Keywords: highway, america, waterways]\n",
      "  Chunk 48 [Keywords: troops, union, america]\n",
      "\n",
      "Standard retrieved 3 chunks, GraphRAG retrieved 12 chunks\n",
      "Chunks added by graph traversal: [2, 3, 4, 5, 6, 7, 9, 13, 48]\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What did Biden say about inflation?\n",
      "================================================================================\n",
      "\n",
      "STANDARD VECTOR SEARCH (similarity only):\n",
      "--------------------------------------------------------------------------------\n",
      " Biden said that inflation is robbing Americans of their gains and that his top priority is getting prices under control. He explained that inflation is caused by a combination of factors, including the pandemic's impact on hiring and global supply chains. He proposed a plan to fight inflation that includes lowering costs, not wages, making more cars and semiconductors in America, investing in infrastructure and innovation, and increasing competition. He also mentioned that his plan would lower the deficit and was supported by Nobel laureates in economics, top business leaders, and most Americans.\n",
      "\n",
      "Retrieved chunks (standard):\n",
      "  Chunk 18 [Keywords: inflation, pandemic, cheaper]\n",
      "  Chunk 24 [Keywords: prosecutor, pandemic, president]\n",
      "  Chunk 19 [Keywords: inflation, economists, insulin]\n",
      "\n",
      "\n",
      "GRAPHRAG (similarity + graph traversal):\n",
      "--------------------------------------------------------------------------------\n",
      " Biden said that inflation is robbing families of the gains they might otherwise feel and that his top priority is getting prices under control. He proposed a plan to lower costs, not wages, by making more cars and semiconductors in America, investing in infrastructure and innovation, and increasing competition. He also mentioned cutting the cost of prescription drugs, child care, and providing more affordable housing, as well as implementing a 15% minimum tax rate for corporations and closing loopholes for the wealthy. Biden emphasized that his plan would lower costs for families and lower the deficit.\n",
      "\n",
      "Retrieved chunks (GraphRAG):\n",
      "  Chunk 18 [Keywords: inflation, pandemic, cheaper]\n",
      "  Chunk 24 [Keywords: prosecutor, pandemic, president]\n",
      "  Chunk 19 [Keywords: inflation, economists, insulin]\n",
      "  Chunk 12 [Keywords: infrastructure, bipartisan, bridges]\n",
      "  Chunk 17 [Keywords: inflation, ford, steelworkers]\n",
      "  Chunk 23 [Keywords: corporations, inflation, taxes]\n",
      "  Chunk 9 [Keywords: kyiv, america, putin]\n",
      "  Chunk 11 [Keywords: growth, deficits, trickle]\n",
      "  Chunk 10 [Keywords: rescue, crisis, president]\n",
      "  Chunk 22 [Keywords: families, taxes, pandemic]\n",
      "  Chunk 13 [Keywords: highway, america, waterways]\n",
      "  Chunk 7 [Keywords: russian, putin, sanctions]\n",
      "\n",
      "Standard retrieved 3 chunks, GraphRAG retrieved 12 chunks\n",
      "Chunks added by graph traversal: [7, 9, 10, 11, 12, 13, 17, 22, 23]\n",
      "\n",
      "================================================================================\n",
      "QUESTION: What did Biden mention about COVID-19?\n",
      "================================================================================\n",
      "\n",
      "STANDARD VECTOR SEARCH (similarity only):\n",
      "--------------------------------------------------------------------------------\n",
      " Biden mentioned that most Americans in most of the country can now be mask-free under new guidelines, and that COVID-19 need no longer control their lives. He emphasized that they will never just accept living with COVID-19 and will continue to combat the virus as they do other diseases. He also highlighted the importance of staying protected with vaccines and treatments, and mentioned that parents with kids under 5 are eager to see a vaccine authorized for their children.\n",
      "\n",
      "Retrieved chunks (standard):\n",
      "  Chunk 32 [Keywords: vaccine, covid, congress]\n",
      "  Chunk 28 [Keywords: covid, vaccine, vaccinating]\n",
      "  Chunk 0 [Keywords: putin, ukrainian, russia]\n",
      "\n",
      "\n",
      "GRAPHRAG (similarity + graph traversal):\n",
      "--------------------------------------------------------------------------------\n",
      " Biden mentioned that the vast majority of Americans have used COVID-19 tools and may want to again, so he expects Congress to pass a request for funding quickly. He also mentioned that under new CDC guidelines, most Americans in most of the country can now be mask-free, and more of the country will reach that point in the next couple of weeks. He emphasized that COVID-19 need no longer control people's lives and that the US will continue to combat the virus as they do other diseases. Biden also highlighted the importance of staying protected with vaccines and treatments, preparing for new variants, and ensuring that no one is left behind or ignored as the country moves forward.\n",
      "\n",
      "Retrieved chunks (GraphRAG):\n",
      "  Chunk 32 [Keywords: vaccine, covid, congress]\n",
      "  Chunk 28 [Keywords: covid, vaccine, vaccinating]\n",
      "  Chunk 0 [Keywords: putin, ukrainian, russia]\n",
      "  Chunk 27 [Keywords: covid, mask, cdc]\n",
      "  Chunk 31 [Keywords: congress, vaccinating, masks]\n",
      "  Chunk 9 [Keywords: kyiv, america, putin]\n",
      "  Chunk 30 [Keywords: provides, testing, vaccines]\n",
      "  Chunk 29 [Keywords: covid, vaccine, antiviral]\n",
      "  Chunk 7 [Keywords: russian, putin, sanctions]\n",
      "  Chunk 1 [Keywords: ambassador, ukrainian, courage]\n",
      "  Chunk 13 [Keywords: highway, america, waterways]\n",
      "  Chunk 42 [Keywords: providing, va, pandemic]\n",
      "\n",
      "Standard retrieved 3 chunks, GraphRAG retrieved 12 chunks\n",
      "Chunks added by graph traversal: [1, 7, 9, 13, 27, 29, 30, 31, 42]\n",
      "\n",
      "✅ Stored final retrieval results for visualization.\n",
      "\n",
      "================================================================================\n",
      "End - RAG Demo: Graph-Enhanced Vector Store with KeybertLinkExtractor\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Suppress tokenizers parallelism warning - needs to be upfront before other imports\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Assuming your environment file is named 'watsonx.env' and is located in the same directory as your script.\n",
    "ENV_FILE_PATH = \"watsonx.env\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# watsonx.ai run-time libraries\n",
    "from ibm_watsonx_ai.credentials import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.graph_vectorstores.extractors import KeybertLinkExtractor\n",
    "from langchain_graph_retriever import GraphRetriever\n",
    "from graph_retriever.strategies import Eager\n",
    "\n",
    "# --- Load environment variables ---\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get credentials\n",
    "WATSONX_API_KEY = os.getenv(\"WATSONX_API_KEY\")\n",
    "WATSONX_URL = os.getenv(\"WATSONX_URL\")\n",
    "WATSONX_PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\")\n",
    "WATSONX_LLM_MODEL_ID = os.getenv(\"WATSONX_LLM_MODEL_ID\")\n",
    "WATSONX_EMBEDDING_MODEL_ID = os.getenv(\"WATSONX_EMBEDDING_MODEL_ID\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "ASTRA_DB_API_ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "\n",
    "# Verify they loaded\n",
    "print(\"=\" * 80)\n",
    "print(\"RAG Demo: Graph-Enhanced Vector Store with KeybertLinkExtractor\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# This block creates the main credential object using the API key and URL.\n",
    "# This object will be used to authenticate all subsequent API calls to watsonx.ai.\n",
    "credentials = Credentials(\n",
    "    url=WATSONX_URL,\n",
    "    api_key=WATSONX_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize foundation model for generation\n",
    "print(\"\\nInitializing watsonx LLM...\")\n",
    "\n",
    "# Initialize the LLM ModelInference client\n",
    "# This block sets up the client for the generative Large Language Model (LLM).\n",
    "# It defines the model to use (WATSONX_LLM_MODEL_ID), the project ID,\n",
    "# and generation parameters (llm_params) like setting temperature to 0.0 for deterministic output.\n",
    "llm_params = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MAX_NEW_TOKENS: 1024,\n",
    "    GenParams.TEMPERATURE: 0.0, # Setting temperature to 0.0 for deterministic RAG answers\n",
    "}\n",
    "\n",
    "watsonx_llm = ModelInference(\n",
    "    model_id=WATSONX_LLM_MODEL_ID,\n",
    "    credentials=credentials,\n",
    "    project_id=WATSONX_PROJECT_ID,\n",
    "    params=llm_params\n",
    ")\n",
    "\n",
    "# Get input file\n",
    "input_file = os.getenv(\"INPUT_FILE\")\n",
    "\n",
    "# 1. Initialize watsonx.ai Embeddings\n",
    "# The WatsonxEmbeddings class reads credentials and project_id from os.environ\n",
    "embedding = WatsonxEmbeddings(\n",
    "    model_id=WATSONX_EMBEDDING_MODEL_ID,\n",
    "    project_id=WATSONX_PROJECT_ID,\n",
    "    url=os.environ[\"WATSONX_URL\"]\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "print(\"Connecting to AstraDB vector store...\")\n",
    "vectorstore = AstraDBVectorStore(\n",
    "    collection_name=\"acme_graphrag_keybert\",\n",
    "    embedding=embedding,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    ")\n",
    "print(f\"Vector store connected\")\n",
    "\n",
    "# Clear existing data for clean reload\n",
    "print(\"Clearing existing data from vector store...\")\n",
    "try:\n",
    "    vectorstore.clear()\n",
    "    print(\"Vector store cleared\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not clear vector store (might be empty): {e}\")\n",
    "\n",
    "# Load the input file\n",
    "print(\"Loading input file...\")\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    speech_text = f.read()\n",
    "print(f\"Loaded {len(speech_text)} characters\")\n",
    "\n",
    "# Split text into chunks\n",
    "print(\"Splitting text into chunks...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_text(speech_text)\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "# Convert chunks to documents (initial docs without keywords)\n",
    "print(\"\\nCreating documents from chunks...\")\n",
    "temp_docs = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    doc = Document(\n",
    "        page_content=chunk,\n",
    "        metadata={\n",
    "            \"source\": input_file,\n",
    "            \"chunk_id\": i,\n",
    "            \"total_chunks\": len(chunks),\n",
    "        }\n",
    "    )\n",
    "    temp_docs.append(doc)\n",
    "\n",
    "# Extract keywords using KeybertLinkExtractor\n",
    "print(\"\\nExtracting keywords with KeyBERT...\")\n",
    "keyword_extractor = KeybertLinkExtractor(kind=\"kw\")\n",
    "\n",
    "# Extract keywords and store as Link objects (standard format for graph extraction)\n",
    "# Store only JSON-safe types in metadata before adding to the Vector Store.\n",
    "docs = []\n",
    "for i, doc in enumerate(temp_docs):\n",
    "    # Extract keywords as Link objects\n",
    "    links = keyword_extractor.extract_one(doc)\n",
    "    \n",
    "    # Convert Link objects to a simple list of keyword strings\n",
    "    keywords = [link.tag for link in links]\n",
    "    \n",
    "    # Create new document with JSON-safe metadata\n",
    "    new_doc = Document(\n",
    "        page_content=doc.page_content,\n",
    "        metadata={\n",
    "            \"source\": input_file,\n",
    "            \"chunk_id\": i,\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"keywords\": keywords,  # LIST of strings - SAFE\n",
    "            # DO NOT include \"links\": links (List of Link objects) here, \n",
    "            # as Link objects are not JSON serializable by AstraDB/AstraPy.\n",
    "        }\n",
    "    )\n",
    "    docs.append(new_doc)\n",
    "    \n",
    "print(f\"Extracted keywords for {len(docs)} documents\")\n",
    "\n",
    "# Show example of extracted keywords\n",
    "print(\"\\nExample document with keywords:\")\n",
    "if docs:\n",
    "    example_doc = docs[0]\n",
    "    print(f\"Chunk {example_doc.metadata['chunk_id']}:\")\n",
    "    print(f\"Content preview: {example_doc.page_content[:100]}...\")\n",
    "    keywords = example_doc.metadata.get('keywords', [])\n",
    "    print(f\"Extracted {len(keywords)} keywords: {keywords[:5]}\")\n",
    "\n",
    "# Add documents to vector store\n",
    "print(f\"\\nAdding {len(docs)} documents to vector store...\")\n",
    "vectorstore.add_documents(docs)\n",
    "print(\"Documents added to vector store\")\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = \"\"\"\n",
    "Here is the context: {context}\n",
    "\n",
    "Based on the context above, answer this question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Helper function to generate response using Watsonx AI\n",
    "def generate_response(context, question):\n",
    "    \"\"\"Generate response using LLM\"\"\"\n",
    "    formatted_prompt = prompt.format(context=context, question=question)\n",
    "    response = watsonx_llm.generate_text(prompt=formatted_prompt)\n",
    "    return response\n",
    "\n",
    "# Questions\n",
    "questions1 = [\n",
    "    \"How is AI used across different industries?\",\n",
    "    \"What are all the climate technologies mentioned?\",\n",
    "    \"What happened after quantum breakthroughs?\" \n",
    "]\n",
    "\n",
    "questions2 = [\n",
    "    \"What did Biden say about Ukraine?\",\n",
    "    \"What did Biden say about inflation?\",\n",
    "    \"What did Biden mention about COVID-19?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNNING COMPARISONS: Standard Vector Search vs GraphRAG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- START OF NEW/MODIFIED CODE BLOCK ---\n",
    "# 1. Initialize variables to store the final results outside the loop scope\n",
    "last_question = \"\"\n",
    "standard_docs_final = []\n",
    "graph_docs_final = []\n",
    "\n",
    "for i, question in enumerate(questions2):\n",
    "    # This index check determines if it is the last question in the list\n",
    "    is_last_question = (i == len(questions2) - 1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ==================== STANDARD VECTOR SEARCH ====================\n",
    "    print(\"\\nSTANDARD VECTOR SEARCH (similarity only):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    standard_retriever = vectorstore.as_retriever(\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    # Retrieve documents\n",
    "    standard_docs = standard_retriever.invoke(question)\n",
    "    \n",
    "    # Format context and generate response\n",
    "    standard_context = format_docs(standard_docs)\n",
    "    standard_response = generate_response(standard_context, question)\n",
    "    \n",
    "    print(standard_response)\n",
    "    \n",
    "    print(\"\\nRetrieved chunks (standard):\")\n",
    "    for i, doc in enumerate(standard_docs, 1):\n",
    "        chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "        keywords = doc.metadata.get('keywords', [])[:3]\n",
    "        keywords_str = f\" [Keywords: {', '.join(keywords)}]\" if keywords else \"\"\n",
    "        print(f\"  Chunk {chunk_id}{keywords_str}\")\n",
    "    \n",
    "    # ==================== GRAPHRAG WITH GRAPH RETRIEVER ====================\n",
    "    print(\"\\n\\nGRAPHRAG (similarity + graph traversal):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create GraphRetriever\n",
    "    # Edge format is: (metadata_field, metadata_field_to_match)\n",
    "    # We're saying: \"follow documents that share keywords\"\n",
    "    graph_retriever = GraphRetriever(\n",
    "        store=vectorstore,\n",
    "        edges=[(\"keywords\", \"keywords\")],  # Connect docs with shared keywords\n",
    "        strategy=Eager(\n",
    "            start_k=3,      # Initial vector search results\n",
    "            k=12,           # Max total results after graph traversal\n",
    "            max_depth=2     # Two hops in the graph\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Retrieve documents using graph traversal\n",
    "    graph_docs = graph_retriever.invoke(question)\n",
    "    \n",
    "    # Format context and generate response\n",
    "    graph_context = format_docs(graph_docs)\n",
    "    graph_response = generate_response(graph_context, question)\n",
    "    \n",
    "    print(graph_response)\n",
    "    \n",
    "    print(\"\\nRetrieved chunks (GraphRAG):\")\n",
    "    for i, doc in enumerate(graph_docs, 1):\n",
    "        chunk_id = doc.metadata.get('chunk_id', 'N/A')\n",
    "        keywords = doc.metadata.get('keywords', [])[:3]\n",
    "        keywords_str = f\" [Keywords: {', '.join(keywords)}]\" if keywords else \"\"\n",
    "        print(f\"  Chunk {chunk_id}{keywords_str}\")\n",
    "    \n",
    "    print(f\"\\nStandard retrieved {len(standard_docs)} chunks, GraphRAG retrieved {len(graph_docs)} chunks\")\n",
    "    \n",
    "    # Show which chunks were added by graph traversal\n",
    "    standard_chunk_ids = {doc.metadata.get('chunk_id') for doc in standard_docs}\n",
    "    graph_chunk_ids = {doc.metadata.get('chunk_id') for doc in graph_docs}\n",
    "    added_by_graph = graph_chunk_ids - standard_chunk_ids\n",
    "    if added_by_graph:\n",
    "        print(f\"Chunks added by graph traversal: {sorted(added_by_graph)}\")\n",
    "    else:\n",
    "        print(f\"No chunks added by graph traversal\")\n",
    "\n",
    "    # Store the results only for the last question\n",
    "    if is_last_question:\n",
    "        last_question = question\n",
    "        standard_docs_final = standard_docs\n",
    "        graph_docs_final = graph_docs\n",
    "        print(\"\\n✅ Stored final retrieval results for visualization.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"End - RAG Demo: Graph-Enhanced Vector Store with KeybertLinkExtractor\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89222e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be74d52d-6a3d-4680-91b8-1d6bdbb1ef8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "Interactive graph saved to graph_rag_results.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"750px\"\n",
       "            src=\"graph_rag_results.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3176201a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from langchain_core.documents import Document\n",
    "from IPython.display import IFrame, display\n",
    "import json # Import json to handle the options dictionary\n",
    "\n",
    "def visualize_graph_rag_comparison_pyvis(question, standard_docs, graph_docs, filename=\"graph_rag_results.html\"):\n",
    "    \"\"\"\n",
    "    Generates an interactive pyvis network graph visualizing the chunks retrieved \n",
    "    by the standard search vs those linked by the GraphRAG approach, \n",
    "    and saves it to an HTML file using net.set_options() for the title/legend.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Setup Data Structures (Retrieval and Keyword Logic)\n",
    "    standard_chunk_ids = {doc.metadata.get('chunk_id') for doc in standard_docs}\n",
    "    chunk_data = {\n",
    "        doc.metadata.get('chunk_id'): {\n",
    "            'keywords': doc.metadata.get('keywords', []),\n",
    "            'is_standard': doc.metadata.get('chunk_id') in standard_chunk_ids,\n",
    "            'doc_content': doc.page_content \n",
    "        }\n",
    "        for doc in graph_docs\n",
    "    }\n",
    "\n",
    "    # 2. Build the NetworkX Graph\n",
    "    G_nx = nx.Graph()\n",
    "    chunk_ids = list(chunk_data.keys())\n",
    "    for chunk_id, data in chunk_data.items():\n",
    "        G_nx.add_node(chunk_id, keywords=data['keywords'], is_standard=data['is_standard'])\n",
    "\n",
    "    # Add edges based on shared keywords\n",
    "    for i in range(len(chunk_ids)):\n",
    "        for j in range(i + 1, len(chunk_ids)):\n",
    "            id1 = chunk_ids[i]\n",
    "            id2 = chunk_ids[j]\n",
    "            keywords1 = set(chunk_data[id1]['keywords'])\n",
    "            keywords2 = set(chunk_data[id2]['keywords'])\n",
    "            shared_keywords = keywords1.intersection(keywords2)\n",
    "            if shared_keywords:\n",
    "                G_nx.add_edge(id1, id2, shared_keywords=\", \".join(list(shared_keywords)[:3]))\n",
    "\n",
    "    # 3. Initialize Pyvis Network\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", notebook=True)\n",
    "    net.force_atlas_2based()\n",
    "\n",
    "    # 4. Transfer Nodes and Edges from NetworkX to Pyvis\n",
    "    STANDARD_COLOR = '#39A7D8'\n",
    "    GRAPH_ADDED_COLOR = '#FF6347'\n",
    "\n",
    "    for node_id in G_nx.nodes():\n",
    "        data = G_nx.nodes[node_id]\n",
    "        is_standard = data['is_standard']\n",
    "        color = STANDARD_COLOR if is_standard else GRAPH_ADDED_COLOR\n",
    "        \n",
    "        keywords_preview = ', '.join(data['keywords'][:5])\n",
    "        content_preview = chunk_data[node_id]['doc_content'][:150].replace('\\n', ' ')\n",
    "        \n",
    "        title_text = f\"Source: {'Initial Search (k=3)' if is_standard else 'Graph Traversal Added'}\\n\"\n",
    "        title_text += f\"Keywords: {keywords_preview}...\\n\"\n",
    "        title_text += f\"Content: {content_preview}...\"\n",
    "        \n",
    "        net.add_node(n_id=node_id, \n",
    "                     label=f\"Chunk {node_id}\",\n",
    "                     color=color,\n",
    "                     title=title_text,\n",
    "                     size=30)\n",
    "\n",
    "    for source, target, data in G_nx.edges(data=True):\n",
    "        edge_title = f\"Shared Keywords: {data.get('shared_keywords', 'N/A')}\"\n",
    "        net.add_edge(source, target, \n",
    "                     title=edge_title, \n",
    "                     color='lightgray', \n",
    "                     width=2)\n",
    "\n",
    "    # 5. Create HTML Heading/Legend Content\n",
    "    # Use standard HTML with inline styling for the legend block\n",
    "    html_heading_content = f\"\"\"\n",
    "    <div style=\"background: rgba(0, 0, 0, 0.8); padding: 20px; border-radius: 8px; color: white; border: 1px solid gray; position: absolute; top: 10px; left: 10px; z-index: 1000;\">\n",
    "        <h2 style=\"color: white; margin-top: 0;\">GraphRAG Comparison: {question}</h2>\n",
    "        <p><strong>Node Color Key:</strong></p>\n",
    "        <p style=\"color: {STANDARD_COLOR};\">■ Initial Vector Search Chunks (Start-k=3)</p>\n",
    "        <p style=\"color: {GRAPH_ADDED_COLOR};\">■ Chunks Added by Graph Traversal</p>\n",
    "        <p><em>Hover over a chunk for details; hover over an edge to see shared keywords.</em></p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 6. Inject HTML using set_options() for compatibility (Fixes TypeError)\n",
    "    \n",
    "    # Set the graph title/legend using the core vis.js 'html' option\n",
    "    # Note: This is an undocumented workaround often required in pyvis\n",
    "    options = {\n",
    "        \"html\": html_heading_content,\n",
    "        \"physics\": {\n",
    "            \"stabilization\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Since net.set_options() expects a JSON string, we need to convert the dict\n",
    "    net.set_options(json.dumps(options))\n",
    "\n",
    "    # 7. Save and Display\n",
    "    net.save_graph(filename) # No 'heading' argument needed now\n",
    "    print(f\"Interactive graph saved to {filename}\")\n",
    "    \n",
    "    return IFrame(src=filename, width='100%', height='750px')\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "# To test this function, you need to call it with actual data.\n",
    "# Assuming you run your RAG loop, you would call it like this:\n",
    "#\n",
    "\n",
    "last_question = \"What did Biden mention about COVID-19?\" # from your questions2 list\n",
    "output = visualize_graph_rag_comparison_pyvis(last_question, standard_docs_final, graph_docs_final)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2ca27-fe5e-4f9e-b15c-ccd7a0290cf1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
